<h1>Human AI  Emotionally Intelligent Chat Application </h1>

 <h4>Project Overview</h4>
<p>In the realm of digital communication, enhancing user experience through emotionally intelligent interactions is crucial. Human AI introduces an advanced chat application that integrates artificial intelligence (AI) and realtime emotion detection to deliver personalized and empathetic user interactions.
</p>
<p>By leveraging facial emotion recognition, the application identifies users' emotional states and adapts its responses accordingly. This approach makes interactions more engaging, effective, and humanlike.
</p>
 <b>Features</b>
 <ul>
   <li>Emotion Chat: Engage in conversations where the AI adapts its responses based on the user's emotional state.</li>
   <li>RealTime Emotion Detection: Continuously monitor facial expressions, displaying detected emotions and dynamically adjusting the conversation flow.</li>
   <li>Speech Recognition & TexttoSpeech: Multimodal communication via speech input and auditory feedback.</li>
   <li> Accurate Emotion Detection: A convolutional neural network (CNN) model identifies emotions such as happiness, sadness, anger, and surprise from facial expressions.</li>
   <li> Contextual Response Generation: Integration with Google Gemini API to generate contextually appropriate responses.
</li>
 </ul>
 

 


<b>Technologies Used</b>
<ul>
<li> Streamlit: For creating the user interface and running the application.</li>
 <li>OpenCV: For capturing and processing realtime facial expressions.</li>
 <li>Keras: For building and deploying the CNN model for emotion detection.</li>
 <li>Google Gemini API: For generating AIdriven, emotionallyaware responses.</li>
 <li>Speech Recognition & TexttoSpeech: For enabling voice input and auditory responses.</li>
</ul>

 <h4>How It Works</h4>
 <p></p>Emotion Detection: Using OpenCV and a pretrained CNN model, the application detects facial expressions and classifies them into predefined emotions.<br>
 Adaptive AI: Based on the detected emotion, the Google Gemini API generates empathetic and contextually relevant responses.<br>
 Multimodal Interaction: Users can communicate with the AI via text or voice input, with responses provided either visually or through texttospeech.</p>

 <h4>Future Enhancements</h4>
 <p>Support for additional emotions and more nuanced detection.<br>
 Integration with external platforms for enhanced realtime communication.<br>
 Improved personalization and adaptability of AI responses.</p>


